#Adding SDETools to $PATH
export PATH=/apollo/env/envImprovement/bin:/apollo/env/SDETools/bin:/usr/local/Cellar/node/5.11.0/bin:~/bin:$PATH:~/ws/misc/src/AmaSSH/scripts:/apollo/env/BrazilThirdPartyTool/bin/:/apollo/env/OctaneBrazilTools/bin:/apollo/env/AmazonAwsCli/bin/:~/.toolbox/bin/

export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home


#Configuration needed by perforce to function correctly in any directory 
 export P4CONFIG=.p4config
 #Make SDETools use a simplified directory structure for organizing source code.  
export BRAZIL_WORKSPACE_DEFAULT_LAYOUT=short
export DISPLAY=:0
# set this to whereever you installed spark
export SPARK_HOME="$HOME/Downloads/spark-2.2.1-bin-hadoop2.7"
export NODE_PATH=/usr/local/Cellar/node/5.11.0/lib/node_modules

# Where you specify options you would normally add after bin/pyspark
export PYSPARK_SUBMIT_ARGS="--master local[2]"
export PATH=$PATH:$SPARK_HOME/bin
export PYSPARK_DRIVER_PYTHON=jupyter
export PYSPARK_DRIVER_PYTHON_OPTS='notebook' pyspark
export PYSPARK_PYTHON=python3.6
export SPARK_LOCAL_IP=127.0.0.1

alias sudome='sudo sed -ri "s/(raghunr[^,]*)(.*)/\1/" /etc/sudoers'
alias bb='brazil-build'
alias b='brazil'
alias ws='brazil ws'
alias gll='(`g config --get remote.origin.url` && g fetch) && gl'
alias ws-co='ws --use --package' 
alias grep-failures='grep -B 2 -E "(Failures|Errors): +[1-9]"'
alias brc='brazil-recursive-cmd'
alias brca='brc --allPackages'
alias bba='brca brazil-build --'
alias bbs='bb server'
alias bbb='bb build'
alias bbc='bb clean'
alias bbcb='bbc && bbb'
alias cssh="ssh -A -t corp-ssh-bastions-36001.sea32.amazon.com ssh -A"
alias ts-node='node ~ws/fortunecookie/src/FortuneCookieWebApp/node_modules/ts-node/dist/bin.js'
alias sync-ws='NODE_PATH=/usr/local/Cellar/node/5.11.0/lib/node_modules node ~/bin/sync.js'
alias prsync='git-root && rsync -avz . d:$RPWD --exclude .git --exclude target --exclude build'
alias ws-root="ws show G 'Root:' A '{print \$2}'"
alias bbjava='bb && brazil-java'
alias brazil-java='java  -cp  `brazil-path run.classpath`'

if [[ '' != `alias open` ]] ; then unalias open; fi
#export AWS_CREDENTIALS_ODIN='com.amazon.access.AutoPnL Beta-dev-raghunr-1'
export ws=~/ws
export down=~/Downloads
export p=~ws/presto/src/Aws157Presto
export ia=~ws/presto/src/Aws751InstanceAgent



emr_extract_logs() {
if [ -z $BASEDIR  ]; then
logs=s3://fortune-analytics-devs/$USER/emr_log/
else
logs=$BASEDIR
fi

echo "using base $logs [Use BASEDIR=your-dir $0 ... to override]"

if [ -z $1  ]; then
 echo "Need cluster id as param to start. Use one of the following: "
 aws s3 ls $logs | sed -E 's#.*PRE ([^/]+)/#\1#'
 return
fi

cluster=$1
clusterbase=${logs}${cluster}/containers/
if [ -z $2 ]; then
 echo "Specify one of the following app ids as the second params: "
 aws s3 ls  $clusterbase | sed -E 's#.*application_([^/]+)/#\1#'
 return
fi
app=$2
s3base=${clusterbase}application_$app/;

nodes=$(aws s3 ls $s3base | wc -l)

mkdir -p ~/logs/$cluster/$app
for ((i=1;i<$nodes;i++)); do
  j=$i
  if [ ${#i} -lt 2 ]; then j=0$i; fi
  aws s3 cp ${s3base}container_${app}_01_0000$j/stderr.gz ~/logs/$cluster/$app/stderr$j.gz
  aws s3 cp ${s3base}container_${app}_01_0000$j/stdout.gz ~/logs/$cluster/$app/stdout$j.gz
done

echo "logs avialable in ~/logs/$cluster/$app"
}

alias emr-socks-prod='emr socks --key-pair-file ~/.ssh/FortuneAnalyticsDataPipeline-prod.pem --cluster-id'
alias emr-socks='emr socks --key-pair-file ~/.ssh/FortuneAnalyticsDataPipeline-beta.pem --cluster-id'

alias emr-ssh-prod='emr ssh --key-pair-file ~/.ssh/FortuneAnalyticsDataPipeline-prod.pem --cluster-id'
alias emr-ssh='emr ssh --key-pair-file ~/.ssh/id_rsa --cluster-id'

alias odin='ssh -L 2009:localhost:2009 d -f -N'
#alias aaa='ssh -L 2009:localhost:2009 d'
alias aaa-fc="socat 'UNIX-LISTEN:/tmp/be.sock,reuseaddr,fork' EXEC:'ssh d socat STDIO UNIX-CONNECT\:\$(cat /apollo/env/FortuneCookie.CONSUMES.AAASecurityDaemon/var/state/aaa/be.sock.txt)' & disown >/dev/null 2>&1"
alias aaa-fc-integration='aaa-fc && odin'

alias s3dp='s3 --profile dp'

alias ssh-with-role=/Users/raghunr/ws/ssh-with-role/src/IotSshUtils/configuration/zsh-scripts/ssh-with-role
alias ssh-with-role-new=/Users/raghunr/ws/athena-misc/src/AthenaBaseLpt/configuration/bin/ssh-with-role

alias presto-cli='~/ws/presto/src/Aws157Presto/presto-cli/target/presto-cli-*-executable.jar'

 source $HOME/.cargo/env


if [ -d ~/bin/isen-cred ]; then
    source ~/bin/isen-cred
fi

bbst() {
bb single-test -DtestClass=$1
}

alias jshell='JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk-11.0.4.jdk/Contents/Home jshell'

source ~/bin/isen-cred
alias emr-log-access='isen_cred 514483059857 EMRSupportLogAccess'

ssh-mf-private() {
    ssh -i ~/asset-iad.pem hadoop@`aws ec2 describe-instances   --filters "Name=network-interface.private-dns-name,Values=$1" "Name=tag-key,Values=Name,Name=tag-value,Values=mf_asset" "Name=instance-state-name,Values=running" --query "Reservations[].Instances[].PublicIpAddress" --output text   | tr '\t' '\n'`
}

export PGHOST=dev-dsk-raghunr-1b-e3f7514d-pg.cbehw6ahzlbi.us-west-2.rds.amazonaws.com
export PGPORT=8192
export PGDATABASE=dca1usw2
export PGUSER=cm_admin
export PGPASSWORD=Cookiemonster1234

export JAVA_HOME=/apollo/env/JavaSE8/jdk1.8/
export PATH=${PATH}:${JAVA_HOME}/bin
export JAVA_CMD=${JAVA_HOME}/bin/java

export XEN_ROOT=~/padb
export XEN_REL=$XEN_ROOT/rel
export PGDATA=~/padb/data/pg
export PATH=~/padb/rel/bin:$PATH

eval $(/home/linuxbrew/.linuxbrew/bin/brew shellenv)

alias rdm="rdm --rp-nice-value 999 -I${HOME}/padb/build/.ec/rootfs/rds/bin/gcc-7.3.0/include/ -I${HOME}/padb/build/.ec/rootfs/rds/bin/gcc-7.3.0/include/c++/7.3.0 -I${HOME}/padb/build/.ec/rootfs/rds/bin/gcc-7.3.0/include/c++/7.3.0/x86_64-pc-linux-gnu -I${HOME}/padb/build/.ec/rootfs/usr/local/include -I${HOME}/padb/build/.ec/rootfs/usr/include -I${HOME}/padb/build/.ec/rootfs/usr/include/libxml2"

export JAVA_HOME=/usr/java/jdk1.8.0_91
source ~/dory/bin/impala-config.sh
